% ============================================================
% AUTO-GENERATED LaTeX — IRL Consumer Demand Recovery
% N_RUNS = 5, N = 800 per run.  All cells: mean (SE).
% Required: booktabs, threeparttable, graphicx, amsmath
% ============================================================

% --- TABLE 1: Predictive Accuracy ---
\begin{table}[htbp]
  \centering
  \caption{Post-Shock Predictive Accuracy: CES Ground Truth, 20\% Fuel Price Shock. Mean (SE) across 5 runs.}
  \label{tab:accuracy}
  \begin{threeparttable}
  \begin{tabular}{lcc}
    \toprule
    \textbf{Model} & \textbf{RMSE} & \textbf{MAE} \\
    \midrule
    LA-AIDS & 0.00979 (0.00010) & 0.00658 (0.00011) \\
    BLP (IV) & 0.06312 (0.00036) & 0.04744 (0.00035) \\
    Lin IRL Shared & 0.15411 (0.00030) & 0.14095 (0.00028) \\
    Lin IRL GoodSpec & 0.15532 (0.00040) & 0.14154 (0.00034) \\
    Lin IRL Orth & 0.02398 (0.00043) & 0.01776 (0.00035) \\
    Neural IRL & \textbf{0.00029 (0.00003)} & 0.00019 (0.00001) \\
    Var. Mixture & 0.03800 (0.00005) & 0.03373 (0.00004) \\
    \bottomrule
  \end{tabular}
  \begin{tablenotes}\small
    \item Mean (SE) across 5 independent draws of $(p,y)$ with $N=800$ each.
    Shock: $p_1\to 1.2\,p_1$. SE $=\hat{\sigma}/\sqrt{n_{\text{runs}}}$. Bold = lowest mean RMSE.
  \end{tablenotes}
  \end{threeparttable}
\end{table}

% --- TABLE 2: Own-Price Elasticities ---
\begin{table}[htbp]
  \centering
  \caption{Recovered Own-Price Elasticities $\hat{\varepsilon}_{ii}$ at Shock Point. Mean (SE).}
  \label{tab:elasticities}
  \begin{threeparttable}
  \begin{tabular}{lccc}
    \toprule
    \textbf{Model} & Food $\hat{\varepsilon}_{00}$ & Fuel $\hat{\varepsilon}_{11}$ & Other $\hat{\varepsilon}_{22}$ \\
    \midrule
    \textit{Ground Truth} & -1.438 (0.001) & -1.489 (0.001) & -1.710 (0.001) \\
    LA-AIDS & -1.422 (0.001) & -1.487 (0.001) & -1.697 (0.003) \\
    BLP (IV) & -1.351 (0.004) & -1.505 (0.004) & -1.000 (0.000) \\
    Lin IRL Shared & -1.522 (0.008) & -1.613 (0.012) & -1.519 (0.013) \\
    Lin IRL GoodSpec & -1.560 (0.007) & -1.703 (0.011) & -1.555 (0.014) \\
    Lin IRL Orth & -1.515 (0.006) & -1.688 (0.006) & -1.838 (0.004) \\
    Neural IRL & -1.438 (0.001) & -1.488 (0.002) & -1.711 (0.001) \\
    Var. Mixture & -1.382 (0.001) & -1.436 (0.001) & -1.574 (0.001) \\
    \bottomrule
  \end{tabular}
  \begin{tablenotes}\small
    \item Mean (SE) across 5 runs. Numerical elasticities at mean post-shock prices, $y=\pounds 1{,}600$.
  \end{tablenotes}
  \end{threeparttable}
\end{table}

% --- TABLE 3: Welfare ---
\begin{table}[htbp]
  \centering
  \caption{Compensating Variation: CS Loss from 20\% Fuel Price Shock. Mean (SE).}
  \label{tab:welfare}
  \begin{threeparttable}
  \begin{tabular}{lcc}
    \toprule
    \textbf{Model} & \textbf{CS Loss (£)} & \textbf{Error (\%)} \\
    \midrule
    \textit{Ground Truth} & £122.62 (0.29) & \text{---} \\
    LA-AIDS & £122.09 (0.29) & 0.4 \\
    BLP (IV) & £120.88 (0.35) & 1.4 \\
    Lin IRL Shared & £92.54 (0.22) & 24.5 \\
    Lin IRL GoodSpec & £92.09 (0.24) & 24.9 \\
    Lin IRL Orth & £121.33 (0.43) & 1.1 \\
    Neural IRL & £122.62 (0.29) & 0.0 \\
    Var. Mixture & £113.93 (0.27) & 7.1 \\
    \bottomrule
  \end{tabular}
  \begin{tablenotes}\small
    \item Mean (SE) across 5 runs. CV via 100-step Riemann integration. Error: \% deviation from Ground Truth mean.
  \end{tablenotes}
  \end{threeparttable}
\end{table}

% --- TABLE 4: Robustness Across DGPs ---
\begin{table}[htbp]
  \centering
  \caption{Out-of-Sample RMSE Across Utility DGPs (Post-Shock). Mean (SE).}
  \label{tab:robustness}
  \begin{threeparttable}
  \begin{tabular}{lccc}
    \toprule
    \textbf{DGP} & \textbf{LA-AIDS} & \textbf{Lin IRL (Orth)} & \textbf{Neural IRL} \\
    \midrule
    CES & 0.00979 (0.00010) & 0.02398 (0.00043) & \textbf{0.00096 (0.00012)} \\
    Quasilinear & 0.00003 (0.00000) & 0.00908 (0.00002) & \textbf{0.00001 (0.00000)} \\
    Leontief & 0.01884 (0.00019) & 0.02658 (0.00046) & \textbf{0.00091 (0.00007)} \\
    Stone-Geary & 0.00581 (0.00004) & 0.00330 (0.00003) & \textbf{0.00232 (0.00004)} \\
    Habit & \textbf{0.04625 (0.00116)} & 0.04796 (0.00119) & 0.04637 (0.00117) \\
    \bottomrule
  \end{tabular}
  \begin{tablenotes}\small
    \item Mean (SE) across 5 runs. Models re-trained per DGP. Bold = lowest mean RMSE per row.
  \end{tablenotes}
  \end{threeparttable}
\end{table}

% --- TABLE 5: MDP Advantage ---
\begin{table}[htbp]
  \centering
  \caption{MDP State Augmentation: Habit Formation Experiment. Mean (SE).}
  \label{tab:mdp_advantage}
  \begin{threeparttable}
  \begin{tabular}{lccl}
    \toprule
    \textbf{Model} & \textbf{RMSE} & \textbf{KL Div.} & \textbf{RMSE reduction} \\
    \midrule
    LA-AIDS (static) & 0.04625 (0.00116) & 0.00887 (0.00039) & baseline \\
    Neural IRL (static MDP) & 0.04622 (0.00118) & 0.00867 (0.00039) & 0.1\% (3.6\%) \\
    MDP Neural IRL ($\bar{x}$ state) & 0.00378 (0.00037) & 0.00006 (0.00001) & 91.8\% (0.8\%) \\
    \bottomrule
  \end{tabular}
  \begin{tablenotes}\small
    \item Mean (SE) across 5 runs. All models trained on identical habit-formation data.
    RMSE reduction relative to LA-AIDS mean; SE propagated via delta method.
    $\hat{\beta}$: Neural IRL = 4.142 (0.071); MDP IRL = 4.179 (0.021). $\theta=0.3$, $\delta=0.7$.
  \end{tablenotes}
  \end{threeparttable}
\end{table}

% --- TABLE 6: Variational Mixture Components (representative last run) ---
\begin{table}[htbp]
  \centering
  \caption{Continuous Variational Mixture IRL: Recovered Parameters ($K=6$, representative run). Component parameters vary across runs; dominant component consistently recovers ground truth.}
  \label{tab:mixture}
  \begin{threeparttable}
  \begin{tabular}{ccccccc}
    \toprule
    $k$ & $\hat{\pi}_k$ & $\hat{\alpha}_{\text{food}}$ & $\hat{\alpha}_{\text{fuel}}$ & $\hat{\alpha}_{\text{other}}$ & $\hat{\rho}$ & Type \\
    \midrule
    1 & 0.067 & 0.405 & 0.326 & 0.269 & 0.300 & Balanced \\
    2 & 0.019 & 0.226 & 0.379 & 0.395 & 0.360 & Balanced \\
    3 & 0.056 & 0.332 & 0.354 & 0.314 & 0.420 & Balanced \\
    4 & 0.812 & 0.395 & 0.385 & 0.220 & 0.481 & \textbf{Dominant} \\
    5 & 0.020 & 0.396 & 0.235 & 0.369 & 0.540 & Balanced \\
    6 & 0.027 & 0.266 & 0.400 & 0.334 & 0.600 & Balanced \\
    \midrule
    \textit{Truth} & --- & 0.400 & 0.400 & 0.200 & 0.450 & --- \\
    \bottomrule
  \end{tabular}
  \begin{tablenotes}\small
    \item Gaussian mixture in $(\alpha,\rho)$ CES parameter space; variational EM on $N=300$ obs. $\alpha$ via softmax; $\rho$ via sigmoid.
  \end{tablenotes}
  \end{threeparttable}
\end{table}

% --- TABLE 7: Linear IRL Feature Ablation ---
\begin{table}[htbp]
  \centering
  \caption{Linear MaxEnt IRL Feature Ablation. Mean (SE).}
  \label{tab:linear_ablation}
  \begin{threeparttable}
  \begin{tabular}{lccp{6cm}}
    \toprule
    \textbf{Variant} & \textbf{RMSE} & \textbf{MAE} & \textbf{Feature description} \\
    \midrule
    Shared (original) & 0.15411 (0.00030) & 0.14095 (0.00028) & Shared $[\ln p_i,(\ln p_i)^2,\ln y]$ — same profile all goods \\
    Good-specific & 0.15532 (0.00040) & 0.14154 (0.00034) & Per-good $[\ln\mathbf{p},(\ln p_i)^2,\ln y]$ — heterogeneous response \\
    Orth + Intercepts & \textbf{0.02398 (0.00043)} & 0.01776 (0.00035) & QR-orth.\ prices + per-good one-hot intercept — resolves collinearity \\
    \bottomrule
  \end{tabular}
  \begin{tablenotes}\small
    \item Mean (SE) across 5 runs. 3{,}000 gradient-ascent epochs, $\ell_2=10^{-4}$, $\eta_t=0.05/(1+t/1000)$. Bold = lowest mean RMSE.
  \end{tablenotes}
  \end{threeparttable}
\end{table}

% ============================================================
% FIGURE INCLUSION BLOCKS
% ============================================================
\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/fig1_demand_curves.pdf}
  \caption{Food Budget Share vs Fuel Price — CES Ground Truth}
  \label{fig:demand_curves}
  \begin{figurenotes}
    Mean predicted share (line) $\pm 1$ SE (shaded band) across 5 runs. Neural IRL (blue) tracks the ground truth most closely. Shared Lin IRL collapses under price collinearity; the Orthogonalised variant recovers the monotone response. Shock point (orange dotted) marks the 20\% fuel price increase.
  \end{figurenotes}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/fig2_mdp_advantage.pdf}
  \caption{MDP Neural IRL vs Static Models — Habit Formation DGP, All Three Goods}
  \label{fig:mdp_advantage}
  \begin{figurenotes}
    Mean $\pm 1$ SE across 5 runs. The MDP-IRL (teal), which receives the lagged habit stock $\bar{x}_t$ in its state vector, tracks all three ground-truth curves most closely. The gap is largest for the Food share.
  \end{figurenotes}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/fig3_convergence.pdf}
  \caption{Training Convergence: KL Loss and Learnable Temperature $\hat{\beta}$ (last run)}
  \label{fig:convergence}
  \begin{figurenotes}
    Left: Neural IRL on CES data. Right: MDP Neural IRL on Habit data. $\hat{\beta}$ stabilises rapidly, providing a data-driven rationality estimate.
  \end{figurenotes}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/fig4_robustness_heatmap.pdf}
  \caption{Robustness: Post-Shock RMSE Across DGPs — Mean and $\pm 1$ SE (5 runs)}
  \label{fig:robustness}
  \begin{figurenotes}
    Left: heatmap of mean RMSE (SE in parentheses). Right: grouped bar chart with $\pm 1$ SE error bars. Neural IRL dominates across all smooth DGPs; the MDP advantage on Habit data only materialises when $\bar{x}$ enters the state (Table~\ref{tab:mdp_advantage}).
  \end{figurenotes}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/fig5_mixture_components.pdf}
  \caption{Continuous Variational Mixture IRL: Weights and Parameter Space ($K=6$, last run)}
  \label{fig:mixture}
  \begin{figurenotes}
    Left: mixture weights $\hat{\pi}_k$ with $\hat{\rho}$ annotated. Right: component centres in $(\hat{\alpha}_{\mathrm{food}},\hat{\alpha}_{\mathrm{fuel}})$ space; size $\propto\hat{\pi}_k$; red star = true $\alpha$.
  \end{figurenotes}
\end{figure}

% REQUIRED PREAMBLE:
% \usepackage{booktabs, threeparttable, graphicx, amsmath}
% \newenvironment{figurenotes}{\par\small\textit{Notes:~}}{\par}